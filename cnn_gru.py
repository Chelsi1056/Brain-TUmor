# -*- coding: utf-8 -*-
"""cnn-gru.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rI_I16KnMjWCdr8C7sbe9LyQ-gCKFJcj
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install Kaggle API and download dataset
!pip install -q kaggle
!mkdir ~/.kaggle
!cp '/content/drive/MyDrive/kaggle.json' ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset
!unzip -q brain-tumor-mri-dataset.zip -d brain_mri

# Create directory structure

import os

base_dir = '/content/brain_mri'
train_dir = os.path.join(base_dir, 'Training')
validation_dir = os.path.join(base_dir, 'Testing')

classes = ['glioma', 'meningioma', 'pituitary', 'notumor']

print("Training Data Distribution:")
for cls in classes:
    class_path = os.path.join(train_dir, cls)
    if os.path.exists(class_path):
        print(f"{cls}: {len(os.listdir(class_path))} images")

print("\nValidation Data Distribution:")
for cls in classes:
    class_path = os.path.join(validation_dir, cls)
    if os.path.exists(class_path):
        print(f"{cls}: {len(os.listdir(class_path))} images")

import matplotlib.pyplot as plt
import os
from tensorflow.keras.preprocessing.image import load_img

classes = ['glioma', 'meningioma', 'pituitary', 'notumor']
base_dir = '/content/brain_mri/Training'  # Update path to your dataset

plt.figure(figsize=(10, 10))
for i, cls in enumerate(classes):
    class_dir = os.path.join(base_dir, cls)
    img_path = os.path.join(class_dir, os.listdir(class_dir)[0])  # Pick one image per class
    img = load_img(img_path, target_size=(224, 224))
    plt.subplot(2, 2, i + 1)
    plt.imshow(img)
    plt.title(cls.capitalize())
    plt.axis('off')
plt.tight_layout()
plt.show()

# Third cell
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Normalization function
def normalize_mri(image):
    return (image / 127.5) - 1.0  # Scale to [-1, 1]

# Data generators
train_datagen = ImageDataGenerator(
    preprocessing_function=normalize_mri,
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

test_datagen = ImageDataGenerator(preprocessing_function=normalize_mri)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

# Replace 'test_dir' with 'validation_dir'
test_generator = test_datagen.flow_from_directory(
    validation_dir, # This line was changed
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

# Fourth cell
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GRU, Dense, Reshape, Flatten

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Reshape((26*26, 128)),  # Corrected reshape
    GRU(64, return_sequences=True),
    GRU(32),
    Dense(128, activation='relu'),
    Dense(4, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

# Fifth cell
import tensorflow as tf # Import tensorflow

history = model.fit(
    train_generator,
    epochs=50,
    validation_data=test_generator,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=3), # Now tf is defined and can be used
        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)
    ]
)

from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
import os
import matplotlib.pyplot as plt

datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

# Get a list of image files in the glioma directory
glioma_dir = '/content/brain_mri/Training/glioma'
image_files = [f for f in os.listdir(glioma_dir) if os.path.isfile(os.path.join(glioma_dir, f))]

# Check if there are any images in the directory
if image_files:
    # Load the first image found
    img_path = os.path.join(glioma_dir, image_files[0])
    img = load_img(img_path, target_size=(224, 224))

    # Convert the PIL Image to a NumPy array
    img_array = img_to_array(img)

    # Generate augmented images
    augmented_images = datagen.flow(img_array.reshape((1,) + img_array.shape), batch_size=1)

    # Plot original and augmented images
    plt.figure(figsize=(8, 8))

    # Original image
    plt.subplot(2, 2, 1)
    # Display the image using the NumPy array (img_array)
    plt.imshow(img_array / 255.0)  # Divide NumPy array by 255.0 for normalization
    plt.title("Original Image")

    # Augmented images
    for i in range(3):
        batch = next(augmented_images)
        plt.subplot(2, 2, i + 2)
        plt.imshow(batch[0] / 255.0)
        plt.title(f"Augmented Image {i + 1}")

    plt.tight_layout()
    plt.show()
else:
    print("No images found in the glioma directory.")

from sklearn.metrics import classification_report

report = classification_report(y_true[:len(y_pred)], y_pred[:len(y_pred)], target_names=class_names)
print(report)

# Save report as text file (optional):
with open("classification_report.txt", "w") as f:
    f.write(report)

# Visualize report using Pandas (optional):
import pandas as pd

report_dict = classification_report(y_true[:len(y_pred)], y_pred[:len(y_pred)], target_names=class_names, output_dict=True)
df_report = pd.DataFrame(report_dict).transpose()

print(df_report) # Display as table in notebook

# Enhanced visualization code
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Function to plot enhanced training history
def plot_enhanced_training_history(history):
    # Create figure
    plt.figure(figsize=(20, 10))

    # Plot accuracy
    plt.subplot(2, 2, 1)
    plt.plot(history.history['accuracy'], linewidth=2, marker='o', markersize=3, label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], linewidth=2, marker='o', markersize=3, label='Validation Accuracy')
    plt.title('Model Accuracy', fontsize=16, fontweight='bold')
    plt.ylabel('Accuracy', fontsize=14)
    plt.xlabel('Epoch', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    # Plot loss
    plt.subplot(2, 2, 2)
    plt.plot(history.history['loss'], linewidth=2, marker='o', markersize=3, label='Training Loss')
    plt.plot(history.history['val_loss'], linewidth=2, marker='o', markersize=3, label='Validation Loss')
    plt.title('Model Loss', fontsize=16, fontweight='bold')
    plt.ylabel('Loss', fontsize=14)
    plt.xlabel('Epoch', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    # Plot learning curve (val_accuracy vs. training set size)
    plt.subplot(2, 2, 3)
    max_acc = max(history.history['val_accuracy'])
    max_acc_epoch = history.history['val_accuracy'].index(max_acc)
    plt.axhline(y=max_acc, color='r', linestyle='--',
               label=f'Max Accuracy: {max_acc:.4f} (Epoch {max_acc_epoch+1})')

    plt.plot(range(1, len(history.history['val_accuracy'])+1),
             history.history['val_accuracy'], 'o-', linewidth=2)
    plt.title('Learning Curve (Validation Accuracy)', fontsize=16, fontweight='bold')
    plt.xlabel('Number of Epochs', fontsize=14)
    plt.ylabel('Validation Accuracy', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    # Plot accuracy vs loss
    plt.subplot(2, 2, 4)
    plt.scatter(history.history['loss'], history.history['accuracy'],
                label='Training', alpha=0.7, s=100)
    plt.scatter(history.history['val_loss'], history.history['val_accuracy'],
                label='Validation', alpha=0.7, s=100)
    plt.title('Accuracy vs Loss', fontsize=16, fontweight='bold')
    plt.xlabel('Loss', fontsize=14)
    plt.ylabel('Accuracy', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    # Show the plots
    plt.tight_layout()

    # Save the figure if needed (optional)
    #plt.savefig('training_history.png', dpi=300)

    # Display the plots
    plt.show()

# Function to generate and plot confusion matrix
def plot_confusion_matrix(generator, model):
    # Get class names
    class_names = list(generator.class_indices.keys())

    # Get predictions
    y_pred = []
    y_true = []

    # Reset the generator to ensure correct order of predictions
    generator.reset()

    # Get true labels and predictions
    for i in range(len(generator)):
        x_batch, y_batch = next(generator)  # Get a batch of images and labels
        y_batch_pred = model.predict(x_batch)  # Predict on the batch
        y_pred.extend(np.argmax(y_batch_pred, axis=1))  # Store predicted classes
        y_true.extend(np.argmax(y_batch, axis=1))  # Store true classes

    # Create confusion matrix
    cm = confusion_matrix(y_true[:len(y_pred)], y_pred[:len(y_pred)])

    # Normalize confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # Plot confusion matrices
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9))

    # Plot raw counts
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names,
                yticklabels=class_names, ax=ax1)

    ax1.set_xlabel('Predicted', fontsize=14)
    ax1.set_ylabel('True', fontsize=14)
    ax1.set_title('Confusion Matrix (Counts)', fontsize=16)


     # Plot normalized percentages
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                 xticklabels=class_names,
                 yticklabels=class_names,
                 ax=ax2)

    ax2.set_xlabel('Predicted', fontsize=14)
    ax2.set_ylabel('True', fontsize=14)
    ax2.set_title('Confusion Matrix (Normalized)', fontsize=16)

     # Show the plots
    plt.tight_layout()
     # Save the figure if needed (optional)
     #plt.savefig('confusion_matrix.png', dpi=300)

     # Display the plots
    plt.show()

     # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_true[:len(y_pred)], y_pred[:len(y_pred)], target_names=class_names))

# Usage instructions:
# After training your model and obtaining the `history` object:
# plot_enhanced_training_history(history)

# For confusion matrix visualization:
# Ensure you have a test_generator set up as follows:
# test_generator = ImageDataGenerator(preprocessing_function=<your_preprocessing_function>).flow_from_directory(
#     validation_dir,
#     target_size=(224, 224),
#     batch_size=<your_batch_size>,
#     class_mode='categorical',
#     shuffle=False  # Important for correct predictions order in confusion matrix!
# )
# Then call:
# plot_confusion_matrix(test_generator, model)
plot_enhanced_training_history(history)

test_generator = ImageDataGenerator(preprocessing_function=normalize_mri).flow_from_directory( # Assuming 'normalize_mri' is your preprocessing function
    validation_dir,
    target_size=(224, 224),
    batch_size=32,  # Replace with your desired batch size
    class_mode='categorical',
    shuffle=False  # Important for correct predictions order in confusion matrix!
)

plot_confusion_matrix(test_generator, model)

